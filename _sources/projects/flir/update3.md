# Update 3

#### First Quarter Recap

In the FLIR group we've been working on a real time object detection model with the primary goal of observing the effects of added noise and adversarial attacks on the models accuracy. Much of our first quarter was spent researching the subject matter, getting familiar with the neccesary tools needed to complete the project, and setting up an environment to work under. By the end of the quarter we had a Yolo v5 model (Model we used for reference(https://github.com/ultralytics/yolov5))
trained on the entire FLIR dataset (https://www.flir.com/oem/adas/adas-dataset-form/) and achieved solid metrics to use as a baseline to test against later. We then started to plan out the experiments we hoped to run and devloped a framework to complete it. 

#### Progress this quarter

Our first step was to create a pipeline that we could use to add noise to each image in the dataset and observe the effect it had in our model. Below is a block diagram to vizualize what this pipeline looks like.

```{figure} Block-Diagram.png
---
height: 150px
name: directive-fig
---
Pipeline Block Diagram
```

In this diagram rectangles represent data, hexagons represent processes, and the cylinder represents our model. Before the pipeline we were able to add noise individually to the image and test it on the model but in order observe the effects on the whole dataset we needed a more streamlined process. Esentially we take our raw images and generate noise on the fly before passing them into our model. The model will generate predictions and from there we gather metrics on the accuracy after noise was added and compare to our baseline. We also included the pipeline for adversarial attacks represented by the dashed lines. In this case there is a feedback loop between the model as the attack is updated based on the gradient with respect to the model weights.  

Now that we've created this pipeline we're ready to test different types of noise on the whole dataset. Below are examples of different types of noise we plan to experiment with. 

```{figure} Noise-Gallery-1.png
---
height: 250px
width: 400px
name: directive-fig
---
Example Noise
```
```{figure} Noise-Gallery-2.png
---
height: 150px
name: directive-fig
---
Example Noise
```
Pictured above are two original images from the dataset and those images after five different types of noise were added. Gaussian noise adds values generated from a normal distribution. Poisson noise uses the image pixels as the mean for a poisson process and this generates a noise mask that is added to the image. Salt & Pepper noise takes really low and really high pixel values and changes them to black and white pixels respectively. Speckle noise is multiplicative process that multiplies the image pixels by values from normal distribution and these values are added back to the image. The bad column noise represents a real world issue with infrared in which an entire column goes bad. This was generated by changing a randomly selected column values all to one.


We did a preliminary test of this pipeline with gaussian noise to ensure that it works. Below are the metrics with and without gaussian noise. 

```{figure} NoGaussMetrics.png
---
height: 150px
name: directive-fig
---
Without Gaussian Noise
```
```{figure} GaussMetrics.png
---
height: 150px
name: directive-fig
---
With Gaussian Noise
```

This is not a final test as we haven't configured our paramaters quite yet but we just wanted to prove that it works and it appears that it does. We can see there is a slight degredation in model performance as evident by the lower mAP @.5 scores. 


#### Next steps

Going further we need a way to compare different types of noise. Considering that each type of noise will degrade the images at different levels we can't just do a one to one comparison of model accuracy. We're using an epsilon value to scale the amount of noise from each distribution and will be calculating the L2 norm(the square root of the sum of squared pixel values) between the noisy and original images. We're currently working on a way to find the epsilon values that will generate similar L2 norms between different noise distributions. This will also help us categorize different levels of noise (low,med,high) and compare model performace at these different levels.   
